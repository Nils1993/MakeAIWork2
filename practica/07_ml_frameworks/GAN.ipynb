{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lightning DataModule</h1>\n",
    "\n",
    "<p>A LightningDataModule is simply a collection of PyTorch DataLoaders with the corresponding transforms and downloading/processing steps required to prepare the data in a reproducible fashion. It encapsulates all steps requires to process data in PyTorch:</p>\n",
    "\n",
    "-   Download and tokenize,\n",
    "-   Clean and save to disk\n",
    "-   Load inside Dataset\n",
    "-   Apply transforms\n",
    "-   Wrap inside a DataLoader\n",
    "\n",
    "<p>Importantly, a LightningDataModule is shareable and reusable. That is, it - -  centralizes all data preparation tasks to have a self-contained object that can exactly duplicate a dataset via identical splits and transforms.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let’s create our MNIST LightningDataModule. First, we initialize with some relevant parameters and create the transform object that we will use to process our raw data. Lastly, we create a dictionary which we will pass into our DataLoaders later.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_dir=\"./data\", batch_size=128, num_workers=int(os.cpu_count() / 2)):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1,), (0.3,))\n",
    "        ])\n",
    "\n",
    "        self.dl_dict = {'batch_size': self.batch_size, 'num_workers': self.num_workers}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we define the prepare_data() function. It defines how to download and e.g. tokenize data. Lightning ensures that this method is called with a single process to avoid corrupted data. Here, we simply download our training and testing sets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def prepare_data(self):\n",
    "        datasets.MINST(self.data_dir, train=True, download=True)\n",
    "        datasets.MINST(self.data_dir, train=False, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next up is the setup() function. It defines data operations you might want to perform on every GPU. Use it to define datasets and e.g. split/transform data or build a vocabulary. In our case, we will split the data into training, validation, and testing sets, using our transform defined in the class __init__() function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def setup(self,stage=None):\n",
    "        # Validation data not strictly necessary for GAN but added for completeness\n",
    "\n",
    "        if stage == \"fit\" or stage is None:\n",
    "\n",
    "            mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, 55000, 5000)\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(self.data_dir, train=False, trainsform=self.transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Finally, we define training, validation, test, and predict DataLoaders. Usually the datasets defined in setup() are simply wrapped in DataLoader objects. Note that validation data is not really necessary for GANs given their unusual evaluation protocols, but nevertheless val_dataloader() is added here for completeness. Further, we have not defined a predict DataLoader here, but the process is identical.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## For dataloaders, usually just wrap dataset defined in setup\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, **self.dl_dict)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, **self.dl_dict)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, **self.dl_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building the Lightning Module</h1>\n",
    "\n",
    "<p>Recall from above that the central object in the Lightning workflow is the LightningModule, which encapsulates the entire model ecosystem. Now that we’ve finished defining our LightningDataModule object, we can build our LightningModule. In our case, this ecosystem includes two models - the generator and the discriminator. We’ll define them now, noting that the process is identical to vanilla PyTorch.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building the Discriminator</h1>\n",
    "\n",
    "<p>First, we’ll construct our discriminator as an nn.Module. We’ll use a simple CNN with two convolutional layers followed by a fully connected network to map from 28x28 single channel digit images to classification predictions. Remember, the purpose of the discriminator is to classify images as real or fake, so we only need a single output node, in contrast to the ten required for the digit classification networks often built for MNIST.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Simple CNN\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we define the forward pass for the discriminator. We use max poolings with a kernel size of two followed by ReLU for the convolutional layers, and use sigmoid for the final activation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "\n",
    "            # Flatten the tensor so it can be fed into the FC layers\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            return F.sigmoid(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building the Generator</h1>\n",
    "<p>\n",
    "Similarly, we construct the generator as an nn.Module. We input data points from a latent space which feed into a linear layer to provide us with enough nodes to create 7x7 images with 64 feature maps. We then used transposed convolutions for learnable upsampling, ultimately collapsing the data to a 28x28 single channel image (i.e. the digit image) via a final convolutional layer.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(latent_dim, 7*7*64)\n",
    "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)\n",
    "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)\n",
    "        self.conv = nn.Conv2d(16, 1, kernel_size=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And again, we define the forward pass.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "            # Pass latent space input into linear layer and reshape\n",
    "            x = self.lin1(x)\n",
    "            x = F.relu(x)\n",
    "            x = x.view(-1, 64, 7, 7)\n",
    "\n",
    "            \n",
    "            # Transposed convolution to 16x16 (64 feature maps)\n",
    "            x = self.ct1(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            # Transposed convolution to 34x34 (16 feature maps)\n",
    "            x = self.ct2(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            # Convolution to 28x28 (1 feature map)\n",
    "            return self.conv(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Defining the GAN\n",
    "</h1>\n",
    "<p>\n",
    "Now that we have our traditional PyTorch nn.Module models, we can build our LightningModule. This is where we will see the Lightning approach diverge from the vanilla PyTorch approach.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Initialization\n",
    "</h1>\n",
    "<p>\n",
    "First, we define our initialization function, inputting our latent dimensionality as well as a learning rate and betas for our Adam optimizers. save_parameters() allows us to store these arguments under the self.hparams attribute (also stored in the model checkpoint) for easier reinstantiation after training. Finally, we initialize the generator and discriminator models within our ecosystem, and generate latent space points that we can use to monitor progress. These points will give us a consistent set of data to track how the generator is progressing in mapping from the same set of latent points to images as it learns.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    ## Initialize. Define latent dim, learning rate, and Adam betas \n",
    "    def __init__(self, latent_dim=100, lr=0.0002, \n",
    "\n",
    "                 b1=0.5, b2=0.999, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.generator = Generator(latent_dim=self.hparams.latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "\n",
    "        self.validation_z = torch.randn(8, self.hparams.latent_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Forward Pass and Loss Function\n",
    "</h1>\n",
    "<p>\n",
    "Next, we define the GAN’s forward pass and loss function. Note that using self.generator(z) is preferred over self.generator.forward(z) given that the forward pass is only one component of the calling logic when self.generator(z) is called.\n",
    "\n",
    "We will be using the loss function in two different ways - one as the discriminator loss and one as the generator loss. The first way is to update the discriminator as a classifier in the canonical fashion, inputting real and generated images with the appropriate labels. The second way is to update the generator using the loss from the discriminator on generated images. That is, the better the discriminator is at detecting fake images, the more the generator is updated. More on this later.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Training Step\n",
    "</h1>\n",
    "<p>\n",
    "Next up is defining what occurs during a training step for the GAN. If we are training the generator, we generate images and then get predictions on them via the discriminator. When we calculate the loss, we are importantly using deceptive labels here. That is, despite the fact that the images are fabricated, we label them as real in the loss function. This is because we want them to be classified as real, so the deceptive labels will lead to greater loss when the generated images are (correctly) classified as fake. We also log some of the fabricated images to be viewed in TensorBoard, and output relevant values.\n",
    "\n",
    "There are no special details for training the discriminator. In a very straightforward fashion, we compute the loss on the real images and the fake images (with honest labels) and average them as the discriminator loss. Again we output the relevant parameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real_imgs, _ = batch\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(real_imgs.shape[0], self.hparams.latent_dim)\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            self.generated_imgs = self(z)\n",
    "            predictions = self.discriminator(self.generated_imgs)\n",
    "            g_loss = self.adversarial_loss(predictions, torch.ones(real_imgs.size(0), 1))\n",
    "            \n",
    "\n",
    "            # log sampled images\n",
    "            sample_imgs = self.generated_imgs[:6]\n",
    "            grid = torchvision.utils.make_grid(sample_imgs)\n",
    "            self.logger.experiment.add_image(\"generated_images\", grid, 0)\n",
    "\n",
    "\n",
    "            tqdm_dict = {\"g_loss\": g_loss}\n",
    "            output = OrderedDict({\"loss\": g_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            real_preds = self.discriminator(real_imgs)\n",
    "            real_loss = self.adversarial_loss(real_preds, torch.ones(real_imgs.size(0), 1))\n",
    "\n",
    "            fake_preds = self.discriminator(self(z).detach())\n",
    "            fake_loss = self.adversarial_loss(fake_preds, torch.zeros(real_imgs.size(0), 1)) \n",
    "\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            tqdm_dict = {\"d_loss\": d_loss}\n",
    "            output = OrderedDict({\"loss\": d_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n",
    "            return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Configure Optimizers\n",
    "</h1>\n",
    "<p>\n",
    "Now we can configure our Adam optimizers using the learning rates and betas we saved in self.hparams. We configure one optimizer for the generator and one for the discriminator.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Epoch End\n",
    "</h1>\n",
    "<p>\n",
    "Finally, we define the on_epoch_end() method. It is not strictly necessary, but we use it here to log images so that we can observe training progress across epochs. It is called whenever a training, testing, or validation epoch ends. Note that each of the training, testing, validation, and predict cases have their own epoch_end() functions in case you do not want to perform such a function across the board!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def on_epoch_end(self):\n",
    "        # log sampled images\n",
    "        sample_imgs = self(self.validation_z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Program Code\n",
    "</h1>\n",
    "<p>\n",
    "Now we can write the main program code to utilize all of the components we’ve defined above, which requires just a few simple lines with Lightning.\n",
    "</p>\n",
    "\n",
    "<h1>\n",
    "Lightning Trainer\n",
    "</h1>\n",
    "<p>\n",
    "Recall from above that the LightningModule is not an abstraction layer on top of PyTorch, but simply a reorganization of code. The abstraction in Lightning comes from the Trainer class.  The Trainer class has very straightforward minimal usage, and is the source of many benefits, including:\n",
    "</p>\n",
    "\n",
    "-   The ability to override any automation component\n",
    "-   The omission of hardware references\n",
    "-   The removal of boilerplate code\n",
    "-   The inclusion of under-the-hood best practices via contributors from top AI labs\n",
    "\n",
    "<p>\n",
    "For our purposes, we simply need to pass in a value for the maximum number of training epochs.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "GAN Training\n",
    "</h1>\n",
    "<p>\n",
    "We’ve defined our LightningDataModule and LightningModule above and instantiated the trainer which will operate on our LightningModule. Now all we need to do is instantiate our LightningDataModule and LightningModule and pass them in to the trainer!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNISTDataModule()\n",
    "model = GAN()\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
