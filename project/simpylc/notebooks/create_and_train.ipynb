{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I create my Lidar model.\n",
    "\n",
    "-   To do this I use Pytorch's modules like: nn.Module(), nn.Sequential(), nn.Linear(), etc.\n",
    "-   To start, I create the class LidarBrain with nn.Module between the ().\n",
    "-   Then I define my model, I decided to use nn.Linear() since this is a regression problem.\n",
    "-   I use nn.Sequential() to define the amount of layers and so that I'm able to change the activation function per layer.\n",
    "-   After some experimentation I've found that using the activation function nn.ReLU() worked best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LidarBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputs_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, outputs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lb_model = nn.Sequential(\n",
    "            nn.Linear(inputs_size, hidden1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_size, hidden2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_size, hidden3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden3_size, hidden4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden4_size, hidden5_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden5_size, outputs_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.lb_model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data\n",
    "\n",
    "-   I load in the data and assign X (scanner data) and Y (steering angle). I decided to leave out the target velocity as that would complicate things.\n",
    "-   I also convert the data to a torch.Tensor(), this worked best after first converting it to a np.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"../data/data_3rounds\"\n",
    "\n",
    "data_file = pd.read_table(load_path, sep = \",\", index_col = 0)\n",
    "\n",
    "\n",
    "# Create X data and Y data by converting them to torch.tensor\n",
    "X = torch.Tensor(np.array(data_file.iloc[:, :-2], copy=False))\n",
    "\n",
    "# We decided to leave out the velocity for now.\n",
    "Y = torch.Tensor(np.array(data_file.iloc[:, 16:17], copy=False))\n",
    "\n",
    "print(X)\n",
    "print(Y[0].type())\n",
    "print(data_file.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to train the model!\n",
    "\n",
    "-   During this project I've experimented with a lot of different settings (more about this after saving the model).\n",
    "-   I start by defining the epochs and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epochs and learning rate.\n",
    "epochs = 200\n",
    "learningRate = 0.000095\n",
    "\n",
    "# Define model and amount of neurons per layer.\n",
    "lb = LidarBrain(len(X[0]), 128, 128, 128, 128, 128, len(Y[0]))\n",
    "print(lb)\n",
    "\n",
    "# Create empty list for losses\n",
    "losses = []\n",
    "\n",
    "# Take binary cross entropy as loss function (one output interpreted as binary)\n",
    "lossFunction = nn.MSELoss()\n",
    "\n",
    "# Use stochastic gradient descent as optimizer, use weights and biases of model\n",
    "gradientDescent = torch.optim.SGD(lb.parameters(), lr=learningRate)\n",
    "\n",
    "# Add a loader for faster calculation and being able to use the shuffle=True function (which makes it less likely the model will just remember the track in order).\n",
    "loader = DataLoader(list(zip(X, Y)), shuffle=True, batch_size=10)\n",
    "\n",
    "# Train. I'm making use of the tqdm() library to keep track while it's buisy\n",
    "for i in tqdm(range(epochs)):\n",
    "\n",
    "    # Create empty losses_epoch list so I can .append() per epoch.\n",
    "    losses_epoch = []\n",
    "\n",
    "    for x, y in loader:\n",
    "    \n",
    "        # Reset the gradient delta's\n",
    "        gradientDescent.zero_grad()\n",
    "\n",
    "        # Forward step\n",
    "        yhat = lb(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = lossFunction(yhat, y)\n",
    "        \n",
    "        # Keep track of loss\n",
    "        losses_epoch.append(loss.item())\n",
    "\n",
    "        # Apply gradient descent (via backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Use w += -step * dw * learnRate\n",
    "        gradientDescent.step()\n",
    "\n",
    "    # Append losses per epoch\n",
    "    losses.append(sum(losses_epoch)/len(losses_epoch))\n",
    "\n",
    "print(losses)\n",
    "\n",
    "# Plot the losses (cost) vs the amount of epochs\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='Epoch', ylabel='Cost', title=\"Training Cost\")\n",
    "\n",
    "plt.plot([x for x in range(len(losses))], losses, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model.\n",
    "\n",
    "-   Use pickle to save the model.\n",
    "-   To experiment and try different epochs/learningrate/hidden-layers etc. I just returned to this notebook every time and saved it again to overwrite the model before or to a new name if I wanted to keep the old model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../pickle/LidarBrain.pkl\"\n",
    "\n",
    "pickle.dump(lb.lb_model, open(save_path, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After trying lots and lots of different things my conclusion is that Lidar is to unstable on THIS track to function in a safe way.\n",
    "\n",
    "Things I've tried:\n",
    "- Different Epochs\n",
    "- Different learning rates\n",
    "- Different data sets\n",
    "- Different batch sizes\n",
    "- More/less hiddenlayers\n",
    "- More/less neurons\n",
    "- With/without activation function.\n",
    "- Different max range on the scanner\n",
    "- Change the data by applying the IQR method\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with Sonar\n",
    "\n",
    "-   Since I did lidar first, all I had to do was do the exact same thing with sonar. As I set the input in my model as len(X) all I had to do was copy and paste the lidar version and change a few things so that it is it's own thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonarBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputs_size, hidden1_size, hidden2_size, outputs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sb_model = nn.Sequential(\n",
    "            nn.Linear(inputs_size, hidden1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_size, hidden2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_size, outputs_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.sb_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"../data/sonar_data_3r\"\n",
    "\n",
    "data_file_2 = pd.read_table(load_path, sep = \",\", index_col = 0)\n",
    "\n",
    "\n",
    "# Create X data and Y data by converting them to torch.tensor\n",
    "X_2 = torch.Tensor(np.array(data_file_2.iloc[:, :-2], copy=False))\n",
    "\n",
    "# We decided to leave out the velocity for now.\n",
    "Y_2 = torch.Tensor(np.array(data_file_2.iloc[:, 3:4], copy=False))\n",
    "\n",
    "print(X_2)\n",
    "print(Y_2)\n",
    "print(data_file_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_2 = 100\n",
    "\n",
    "learningRate_2 = 0.00001\n",
    "\n",
    "#----------------------------------------\n",
    "\n",
    "sb = SonarBrain(len(X_2[0]), 64, 64, len(Y_2[0]))\n",
    "\n",
    "print(sb)\n",
    "\n",
    "#----------------------------------------\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Take binary cross entropy as loss function (one output interpreted as binary)\n",
    "lossFunction_2 = nn.MSELoss()\n",
    "\n",
    "# Use stochastic gradient descent as optimizer, use weights and biases of model\n",
    "gradientDescent_2 = torch.optim.SGD(sb.parameters(), lr=learningRate_2)\n",
    "\n",
    "#----------------------------------------\n",
    "\n",
    "for j in tqdm(range(epochs_2)):\n",
    "\n",
    "    losses_epoch = []\n",
    "    for x, y in zip(X_2, Y_2):\n",
    "    \n",
    "        # Reset the gradient delta's (dw, db)\n",
    "        gradientDescent_2.zero_grad()\n",
    "\n",
    "        # Forward step\n",
    "        yhat = sb(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = lossFunction_2(yhat, y)\n",
    "        \n",
    "        # Keep track of loss\n",
    "        losses_epoch.append(loss.item())\n",
    "\n",
    "        # Apply gradient descent (via backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Use w += -step * dw * learnRate\n",
    "        gradientDescent_2.step()\n",
    "\n",
    "    losses.append(sum(losses_epoch)/len(losses_epoch))\n",
    "\n",
    "print(losses)\n",
    "\n",
    "#----------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='Epoch', ylabel='Cost', title=\"Training Cost\")\n",
    "\n",
    "plt.plot([x for x in range(len(losses))], losses, 'red')\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../pickle/SonarBrain.pkl\"\n",
    "\n",
    "pickle.dump(sb.sb_model, open(save_path, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "-   Experimenting with both Lidar and Sonar i've found that Lidar is very unstable compared to Sonar, my sonar model worked almost perfectly after 2 different training sessions while I've worked for almost a whole week to try and make the Lidar version more stable.\n",
    "\n",
    "-   After testing the hardcoded Lidar version some more I found out that after a couple of rounds it also straightup runs off the track. This means I've been training my Lidar model with unstable data to begin with and it won't ever be stable using this type of machinelearning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
